{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70d3e2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# Additional Imports\n",
    "import os, json, math, time\n",
    "from yelpapi import YelpAPI\n",
    "from tqdm.notebook import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9de5685",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<yelpapi.yelpapi.YelpAPI at 0x24bad2abdc8>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load API Credentials\n",
    "with open('/Users/tijes/.secret/yelp_api.json') as f:\n",
    "    login = json.load(f)\n",
    "login.keys()\n",
    "# Instantiate YelpAPI Variable\n",
    "yelp_api = YelpAPI(login['api-key'], timeout_s=5.0)\n",
    "yelp_api"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f6caaf",
   "metadata": {},
   "source": [
    "## Define Search\n",
    "\n",
    "To allow us to easily perform different searches in the future, we will define variables for LOCATION and TERM set for our particular search conditions. Then, when we want to use a different location or term, we can just redefine these variables. This streamlines the code and makes it more readable and reproducible.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fff94c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set our API call parameters and filename before the first call\n",
    "LOCATION = 'Baltimore, MD,21202'\n",
    "TERM = 'Crab Cakes'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af73f929",
   "metadata": {},
   "source": [
    "## Create a results-in-progress JSON file, but only if it doesn't exist.\n",
    "\n",
    "This is the file where your results will be saved.  Note: you must rename your JSON_FILE for different queries to prevent confusing results from other searches. We recommend you include your search terms in the filename. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b1bdfe0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Data/results_in_progress_Crab_cakes.json'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Specifying JSON_FILE filename (can include a folder)\n",
    "# include the search terms in the filename\n",
    "JSON_FILE = f\"Data/results_in_progress_Crab_cakes.json\"\n",
    "JSON_FILE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fba029",
   "metadata": {},
   "source": [
    "Check if our JSON_FILE already exists. This will prevent us from accidently overwriting an existing file.\n",
    "\n",
    "If it doesn't exist:\n",
    "\n",
    "- Create any folders needed for the file path.\n",
    "- Save an empty list as JSON_File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f992f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[i] Data/results_in_progress_Crab_cakes.json already exists.\n"
     ]
    }
   ],
   "source": [
    "## Check if JSON_FILE exists\n",
    "file_exists = os.path.isfile(JSON_FILE)\n",
    "## If it does not exist: \n",
    "if file_exists == False:\n",
    "    \n",
    "    ## CREATE ANY NEEDED FOLDERS\n",
    "    # Get the Folder Name only\n",
    "    folder = os.path.dirname(JSON_FILE)\n",
    "    ## If JSON_FILE included a folder:\n",
    "    if len(folder)>0:\n",
    "        # create the folder\n",
    "        os.makedirs(folder,exist_ok=True)\n",
    "        \n",
    "        \n",
    "    ## INFORM USER AND SAVE EMPTY LIST\n",
    "    print(f\"[i] {JSON_FILE} not found. Saving empty list to file.\")\n",
    "    \n",
    "    \n",
    "    ## save the first page of results\n",
    "    with open(JSON_FILE,'w') as f:\n",
    "        json.dump([],f)  \n",
    "## If it exists, inform user\n",
    "else:\n",
    "    print(f\"[i] {JSON_FILE} already exists.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cd68b9",
   "metadata": {},
   "source": [
    "## Determine how many results are already in the file\n",
    "\n",
    "Load the results file to determine the # of results we have previously retrieved. If you just created the file, you would expect it to be empty.  \n",
    "\n",
    "We will use this as our offset parameter for our API call.  Even if this is your first API call, and the number is 0, we want to define \"n_results\" based on the length of \"previous_results.\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f99fc319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- 424 previous results found.\n"
     ]
    }
   ],
   "source": [
    "## Load previous results and use len of results for offset\n",
    "with open(JSON_FILE,'r') as f:\n",
    "    previous_results = json.load(f)\n",
    "    \n",
    "## set offset based on previous results\n",
    "n_results = len(previous_results)\n",
    "print(f'- {n_results} previous results found.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ca0383",
   "metadata": {},
   "source": [
    "## Figure out how many pages of results we will need\n",
    "\n",
    "We will perform our first query to get our first page of results and the total number of results.  We will then (via code) calculate how many pages we will need to retrieve all of our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2095eb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['businesses', 'total', 'region'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use our yelp_api variable's search_query method to perform our API call\n",
    "results = yelp_api.search_query(location=LOCATION,\n",
    "                                term=TERM,\n",
    "                               offset=n_results+1)\n",
    "results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30146b6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "433"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## How many results total?\n",
    "total_results = results['total']\n",
    "total_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19c76bdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## How many did we get the details for?\n",
    "results_per_page = len(results['businesses'])\n",
    "results_per_page"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c929188",
   "metadata": {},
   "source": [
    "There are over 400 businesses to retrieve from our API and we can get 20 results at a time (per \"page\").\n",
    "\n",
    "- We can calculate the # of results remaining by subtracting our offset (length of our previous results) from our total.\n",
    "\n",
    "- Then we can determining how many pages we will need by dividing the results by 20 (or whatever the value happens to be for results per page)\n",
    "\n",
    "- Note that we need to round up the number of pages in order to get all of the results. Even if there is only 1 result on the last page, we want to include that page! To do this we will use math.ceil. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2e8b185",
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18208\\146334996.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Use math.ceil to round up for the total number of pages of results.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mn_pages\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'total'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mn_results\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m \u001b[0mresults_per_page\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mn_pages\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "# Import additional packages for controlling our loop\n",
    "import time, math\n",
    "# Use math.ceil to round up for the total number of pages of results.\n",
    "n_pages = math.ceil((results['total']-n_results)/ results_per_page)\n",
    "n_pages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b690694f",
   "metadata": {},
   "source": [
    "When this example was written, there were 437 results and 20 results per page.  437 /20 = 21.85. Rounding UP gives us 22 pages.  We expect the first 21 pages to have 20 results and the last page to have the final 17 results.  Notice that we have assigned the number of pages as n_pages here. We will use this value in our next segment of code.\n",
    "\n",
    "You can see that having to manually go through 22 pages would be quite time consuming and inefficient! First we are going to save the first page into our file, and then we will add on to it with a for loop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d961cf79",
   "metadata": {},
   "source": [
    "## Add this page of results to .json file\n",
    "\n",
    "Our API returns our results in JSON format, with the businesses in a list of dictionaries. We will append the first page of businesses to our previous_results (which is very likely empty) and then save to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283aec4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# join new results with old list with extend and save to file\n",
    "previous_results.extend(results['businesses'])  \n",
    "with open(JSON_FILE,'w') as f:\n",
    "     json.dump(previous_results,f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fda1790",
   "metadata": {},
   "source": [
    "## Set up a progress bar in our for loop.\n",
    "\n",
    "To keep us informed about where we are in our loop, we will add a progress bar to our for loop.\n",
    "\n",
    "TQDM is a package designed for adding animated progress bars to Python processes.  It is not currently included in your dojo-env, so you are going to install it manually by opening a new Terminal/GitBash window and running the following command:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13dd15ae",
   "metadata": {},
   "source": [
    "TQDM  is easy to use and simply needs to know what we are looping through. If you wanted to test tqdm in action, but your loop is too fast, you can import time and use time.sleep to add a pause within your for loop. We will also use time.sleep when executing many API calls so that we do not overwhelm the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d337e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm_notebook\n",
    "import time\n",
    "for i in tqdm_notebook(range(n_pages)):\n",
    "    # adds 200 ms pause\n",
    "    time.sleep(.2) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472fdc74",
   "metadata": {},
   "source": [
    "## For Loop to call each page\n",
    "\n",
    "The loop below will iterate through each page of the results by starting at the appropriate offset.  It will then append the results to the previous_results.  This may take some time, so check out the progress bar!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d61914b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm_notebook( range(1,n_pages+1)):\n",
    "    time.sleep(.2)\n",
    "    ## Read in results in progress file and check the length\n",
    "    with open(JSON_FILE, 'r') as f:\n",
    "        previous_results = json.load(f)\n",
    "    ## save number of results for to use as offset\n",
    "    n_results = len(previous_results)\n",
    "    ## use n_results as the OFFSET \n",
    "    results = yelp_api.search_query(location=LOCATION,\n",
    "                                    term=TERM, \n",
    "                                    offset=n_results+1)\n",
    "    \n",
    "    ## append new results and save to file\n",
    "    previous_results.extend(results['businesses'])\n",
    "    \n",
    "#     display(previous_results)\n",
    "    with open(JSON_FILE,'w') as f:\n",
    "        json.dump(previous_results,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2d7030",
   "metadata": {},
   "source": [
    "# After the loop has finished"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8022bd2",
   "metadata": {},
   "source": [
    "## Convert .json to dataframe\n",
    "\n",
    "Load in the \"results in progress\" JSON file into a DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216872ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load final results\n",
    "final_df = pd.read_json(JSON_FILE)\n",
    "display(final_df.head(), final_df.tail())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c702aafd",
   "metadata": {},
   "source": [
    "## Check for duplicates\n",
    "Check IDs to make sure you have unique results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed393688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for duplicate IDs\n",
    "final_df.duplicated(subset='id').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c22fb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the duplicates\n",
    "final_df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e005baa",
   "metadata": {},
   "source": [
    "## Save the final DataFrame to a .csv (or a .csv.gz if its too big for the GitHub file size limit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c52a607b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'final_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18208\\170796389.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# save the final results to a compressed csv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mfinal_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Data/final_results_crab_cakes.csv.gz'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'gzip'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'final_df' is not defined"
     ]
    }
   ],
   "source": [
    "# save the final results to a compressed csv\n",
    "final_df.to_csv('Data/final_results_crab_cakes.csv.gz', compression='gzip',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65013d1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dojo-env)",
   "language": "python",
   "name": "dojo-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
